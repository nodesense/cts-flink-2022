{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a440d1-d759-4b7c-9e1f-fb525b325ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.common import WatermarkStrategy, Row\n",
    "from pyflink.common.serialization import Encoder\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors import FileSink, OutputFileConfig, NumberSequenceSource\n",
    "from pyflink.datastream.functions import RuntimeContext, MapFunction\n",
    "from pyflink.datastream.state import ValueStateDescriptor\n",
    "from pyflink.common.serialization import JsonRowDeserializationSchema\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors import FlinkKafkaConsumer,FlinkKafkaProducer\n",
    "from pyflink.common.serialization import SimpleStringSchema, SerializationSchema,JsonRowSerializationSchema,Encoder\n",
    "from pyflink.common.typeinfo import Types\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab9e6a9-4c6a-4071-bc9a-b10e054903f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/flink-1.15.0/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "# wget -P $FLINK_HOME/lib https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/1.15.0/flink-sql-connector-kafka-1.15.0.jar \n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "# the sql connector for kafka is used here as it's a fat jar and could avoid dependency issues\n",
    "env.add_jars(\"file:///opt/flink-1.15.0/lib/flink-sql-connector-kafka-1.15.0.jar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6a2001-0a27-42a3-81df-a2a27dc53dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "deserialization_schema = JsonRowDeserializationSchema.builder().type_info(\n",
    "                             type_info=Types.ROW_NAMED(\n",
    "                             [\"tag\",\"value\"], [Types.STRING(), Types.DOUBLE()])).build()\n",
    "\n",
    "##Data: {\"abc\": \"123\", \"xyz\": \"ddd\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474d5b9b-0583-40b7-b387-4dd5119945a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect kafka with spark with simple word count example\n",
    "# run on a terminal after starting kafka\n",
    "\n",
    "#     kafka-topics  --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test-json\n",
    "#     kafka-console-producer --bootstrap-server localhost:9092 --topic test-json  \n",
    "\n",
    "# {\"tag\": \"sensor1\", \"value\": 32.3}\n",
    "# {\"tag\": \"sensor2\", \"value\": 34.3}\n",
    "\n",
    "# here we implement windowed word count, the word count would reset every minute, we fix  5 minute window\n",
    "\n",
    "#     kafka-topics  --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test-json-sink\n",
    "#     kafka-console-consumer --bootstrap-server localhost:9092 --topic  test-json-sink  --from-beginning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799313df-d19e-43f0-9292-6c3bed964fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_consumer = FlinkKafkaConsumer(\n",
    "    topics='test-json',\n",
    "    deserialization_schema=deserialization_schema,\n",
    "    properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'json-group-1'})\n",
    "\n",
    "ds = env.add_source(kafka_consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e985204-17b7-4c06-a8a7-61fa58969868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.datastream.data_stream.DataStreamSink at 0x7fa947eca4c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.print()\n",
    "# env.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746716d-caa0-4ef5-b758-077506895c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2> +I[sensor2,34.3]\n"
     ]
    }
   ],
   "source": [
    "serialization_schema = JsonRowSerializationSchema.builder().with_type_info(\n",
    "    type_info=Types.ROW_NAMED([\"tag\",\"value\"],[Types.STRING(), Types.DOUBLE()])).build()\n",
    "\n",
    "kafka_producer = FlinkKafkaProducer(\n",
    "    topic='test-json-sink',\n",
    "    serialization_schema=serialization_schema,\n",
    "    producer_config={'bootstrap.servers': 'localhost:9092'})\n",
    "\n",
    "ds.add_sink(kafka_producer)\n",
    "\n",
    "env.execute('state_access_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d925f-604b-43a1-973d-6bec93dcc9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d202ec-5120-43d3-bb67-2a10a46caa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
